{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "print(tf.__version__)\n",
        "\n",
        "# Set some random seeds so that we should get the same answer! This method\n",
        "# sets the keras, python and numpy random seeds\n",
        "keras.utils.set_random_seed(42)\n",
        "tf.config.experimental.enable_op_determinism()\n",
        "\n",
        "# Define all of the random tensors for the unit tests to ensure we have the same\n",
        "# numbers!\n",
        "unit_test_1_input = np.random.rand(1,5,4)\n",
        "unit_test_2_input = np.random.rand(1,5,4)\n",
        "unit_test_3_input = np.random.rand(1,5,4)\n",
        "unit_test_4_input = np.random.rand(1,5,4)\n",
        "\n",
        "# Get the ANKI English to Spanish dataset\n",
        "text_file = keras.utils.get_file(\n",
        "    fname=\"spa-eng.zip\",\n",
        "    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\",\n",
        "    extract=True,\n",
        ")\n",
        "import pathlib\n",
        "data_path = pathlib.Path(text_file).parent / \"spa-eng\" / \"spa.txt\""
      ],
      "metadata": {
        "id": "8NBJgdkhbCkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(data_path) as data_file:\n",
        "    lines = data_file.read().split(\"\\n\")[:-1]\n",
        "\n",
        "import re\n",
        "# Now convert the lines into English to Spanish pairs\n",
        "# Since this is a simple model, it makes sense to enforce lower case and\n",
        "# remove any punctuation\n",
        "english_spanish_pairs = []\n",
        "max_length_english = 0\n",
        "for line in lines:\n",
        "    line = line.lower()\n",
        "    # Below regex removes characters not in the list, don't forget tabs (\\t)!\n",
        "    # We need to make sure that we keep the special characters from Spanish\n",
        "    line = re.sub(r'[^A-Za-z0-9 \\táéíóúñ]+', '', line)\n",
        "    english, spanish = line.split(\"\\t\")\n",
        "    # As seen in the lecture, we need to add start and end tokens for Spanish\n",
        "    spanish = \"<sos> \" + spanish + \" <eos>\"\n",
        "    english_spanish_pairs.append((english, spanish))\n"
      ],
      "metadata": {
        "id": "d3bP4ih_bIe6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffle the dataset and split into training and validation samples (80%/20%)\n",
        "random.shuffle(english_spanish_pairs)\n",
        "# Let's have a look at a couple of examples\n",
        "print('We have', len(english_spanish_pairs), 'examples in the dataset')\n",
        "for w in range (5):\n",
        "    print('Example',w,':', english_spanish_pairs[w])\n",
        "\n",
        "num_training = int(0.8*len(english_spanish_pairs))\n",
        "training_pairs = english_spanish_pairs[:num_training]\n",
        "validation_pairs = english_spanish_pairs[num_training:]\n",
        "\n",
        "print(\"Training sample size =\", len(training_pairs),\n",
        "      \"and validation sample =\", len(validation_pairs))"
      ],
      "metadata": {
        "id": "wrNcq_0Md8qR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the maximum vocabulary size that we want to use. If there\n",
        "# are more words than this, then the 15000 most frequent words are\n",
        "# used as the vocabulary\n",
        "vocab_size_english = 15000\n",
        "vocab_size_spanish = 15000\n",
        "\n",
        "# Define what the maximum sentence length is. If the sentence has\n",
        "# fewer words then it is padded wih zeros during the vectorisation\n",
        "sequence_length = 20\n",
        "\n",
        "# This layer simply converts a word into an integer\n",
        "english_vectorisation = keras.layers.TextVectorization(\n",
        "                        max_tokens=vocab_size_english,\n",
        "                        output_mode=\"int\",\n",
        "                        output_sequence_length=sequence_length,\n",
        "                        standardize=None)\n",
        "# The start / end tokens expand our sequences by one. It isn't two because\n",
        "# we don't use the <sos> token in the decoder output, or the <eos> token\n",
        "# in the decoder input\n",
        "spanish_vectorisation = keras.layers.TextVectorization(\n",
        "                        max_tokens=vocab_size_spanish,\n",
        "                        output_mode='int',\n",
        "                        output_sequence_length=sequence_length+1,\n",
        "                        standardize=None)\n",
        "\n",
        "# We need to separate out our language pairs to build the vocabulary\n",
        "english_words = []\n",
        "spanish_words = []\n",
        "for pair in english_spanish_pairs:\n",
        "  english_words.append(pair[0])\n",
        "  spanish_words.append(pair[1])\n",
        "\n",
        "# Actually build the vocabulary\n",
        "english_vectorisation.adapt(english_words)\n",
        "spanish_vectorisation.adapt(spanish_words)\n"
      ],
      "metadata": {
        "id": "1JBUGCGigNKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the batch size now since the dataset needs to know\n",
        "batch_size = 64\n",
        "\n",
        "# We need to format our datasets a little more. We actually need three types\n",
        "# of sentences\n",
        "# 1) \"encoder_inputs\": english sentences\n",
        "# 2) \"decoder_inputs\": spanish sentences with <sos> for teacher forcing\n",
        "# 3) target sentence to predict: spanish sentences with <eos> at the end\n",
        "# A simple way to use this data and allow batching is a tf.Data.Dataset object\n",
        "def format_dataset(english, spanish_input, spanish_target):\n",
        "    inputs = {\"encoder_inputs\": english_vectorisation(english),\n",
        "              \"decoder_inputs\": spanish_vectorisation(spanish_input),}\n",
        "    target = spanish_vectorisation(spanish_target)\n",
        "    return (inputs, target,)\n",
        "\n",
        "def make_dataset(pairs, batch_size):\n",
        "    eng_texts, spa_texts = zip(*pairs)\n",
        "    # Decoder input needs to remove the last word (<eos>)\n",
        "    spanish_input = list(spa_texts)\n",
        "    for word in range(len(spanish_input)):\n",
        "        spanish_input[word] = spanish_input[word].rsplit(' ', 1)[0]\n",
        "    # Target needs to remove the first word (<sos>)\n",
        "    spanish_target = list(spa_texts)\n",
        "    for word in range(len(spanish_target)):\n",
        "        spanish_target[word] = spanish_target[word].split(' ', 1)[1]\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((list(eng_texts), spanish_input, spanish_target))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(format_dataset)\n",
        "    return dataset.cache().shuffle(2048).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "training_dataset = make_dataset(training_pairs, batch_size)\n",
        "validation_dataset = make_dataset(validation_pairs, batch_size)"
      ],
      "metadata": {
        "id": "yvZX8iVEm0hH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for inputs, targets in training_dataset.take(1):\n",
        "    print('inputs[\"encoder_inputs\"] shape: ', inputs[\"encoder_inputs\"].shape)\n",
        "    print('inputs[\"decoder_inputs\"] shape: ', inputs[\"decoder_inputs\"].shape)\n",
        "    print('targets shape: ', targets.shape)\n",
        "    print(inputs[\"encoder_inputs\"][0])\n",
        "    print(inputs[\"decoder_inputs\"][0])\n",
        "    print(targets[0])"
      ],
      "metadata": {
        "id": "7fuNT8OWttwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "We have (finally) formatted the training and validation data into a format we can use. Now let's get on with defining all of the components that we need for our transformer.\n",
        "1. Word embedding and position encoding\n",
        "2. Multi-head attention\n",
        "3. Feed-forward network\n",
        "4. The encoder\n",
        "5. The decoder\n",
        "6. The transformer (brings together all of the above)\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "vmkxOSsXsz6S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Firstly, let's make sinusiodal position encoding layer. We will actually\n",
        "# used a learned position encoding, but I wanted to include this function\n",
        "# for completeness as it was used in the original transformer\n",
        "class SinusoidalPositionEncoding(keras.layers.Layer):\n",
        "    def __init__(self, sequence_length, d_model, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.l = sequence_length\n",
        "        self.d_model = d_model\n",
        "\n",
        "    # Inputs doesn't do anything, but needed to make other interfaces\n",
        "    def call(self, inputs):\n",
        "        position = np.arange(self.l)[:, np.newaxis]\n",
        "        # Create div_term array with shape (d_model//2,)\n",
        "        # which corresponds to the denominators raised to the power 2i/d_model\n",
        "        div_term = np.exp(-np.log(10000.0) * (np.arange(0, self.d_model, 2) / self.d_model))\n",
        "        # Initialize the position encoding array\n",
        "        encoded_position = np.zeros((self.l, self.d_model))\n",
        "        # Compute sinusoidal values using broadcasting\n",
        "        encoded_position[:, 0::2] = np.sin(position * div_term)\n",
        "        encoded_position[:, 1::2] = np.cos(position * div_term)\n",
        "        return encoded_position"
      ],
      "metadata": {
        "id": "eMF0R-JefIZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding layer that does the embedding and position encoding. We can choose\n",
        "# whether to use the learned or sinusoidal encoding.\n",
        "class EmbedAndEncode(keras.layers.Layer):\n",
        "    def __init__(self, sequence_length, d_model, vocab_size, learned_encoding, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        # Embedding layers\n",
        "        self.l = sequence_length\n",
        "        self.embedding = keras.layers.Embedding(vocab_size, d_model)\n",
        "        if learned_encoding:\n",
        "            self.position = keras.layers.Embedding(sequence_length, d_model)\n",
        "        else:\n",
        "            self.position = SinusoidalPositionEncoding(sequence_length, d_model)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        e = self.embedding(inputs)\n",
        "        positions = np.arange(0, self.l, 1)\n",
        "        p = self.position(positions)\n",
        "        x = p + e\n",
        "        return x"
      ],
      "metadata": {
        "id": "KBvDI9roeQQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "In this next block we will build the attention mechanism. This is the key component of the transformer. To recap from the lectures, there are a few steps that we need to follow.\n",
        "\n",
        "\n",
        "1.   We get three inputs passed into the attention heads, which then pass through the $W^Q$, $W^K$ and $W^V$ matrices, which all have dimensions $\\left( d_\\textrm{model}, d_k \\right)$ to give us queries $Q$, keys $K$, and values $V$.\n",
        "2.   Calculate $A = \\textrm{softmax}\\left( \\frac{QK^T}{\\sqrt{d_k}} \\right)$\n",
        "3.   The output from the attention head if given by the matrix product $AV$\n",
        "4.   Repeat the above for each attention head and concatenate the output\n",
        "5.   Pass the concatenated output through the dense layer representing matrix $W^0$ with shape $\\left( \\left(n_\\textrm{heads}d_k \\right) \\times d_\\textrm{model}\\right)$\n",
        "\n",
        "Implementation information:\n",
        "\n",
        "*   We saw in the lectures how to represent the weight matrices as `Dense` layers, where the number of neurons is equal to the number of columns in the matrix. We need to ensure that we don't use a bias term and that we are using a linear (effectively the identity) activation function. This is the default in keras so we don't technically need to specify it in the code below.\n",
        "* Use `tf.matmul(A,B)` to multiply matrix $A$ by matrix $B$. It can take optional boolean arguments to transpose the matrices before multiplying (`transpose_a` and `transpose_b`)\n"
      ],
      "metadata": {
        "id": "cUoO5ZB82tgN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ordinarily we could just use keras.layers.Attention or MultiHeadAttention\n",
        "# to build a transformer. However, in this case let's write our own layer\n",
        "# as an exercise.\n",
        "class CustomAttention(keras.layers.Layer):\n",
        "    def __init__(self, d_model, d_k, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        # Important values\n",
        "        self.d_k = d_k\n",
        "        self.num_heads = num_heads\n",
        "        # Q, K and V weight matrices for each head.\n",
        "        # Fill in the number of nodes in the W^Q, W^K and W^V dense layers\n",
        "        # by replacing the \"None\" values in the three lines below\n",
        "        self.Wq = [keras.layers.Dense(d_k, use_bias=False, activation='linear') for _ in range(self.num_heads)]\n",
        "        self.Wk = [keras.layers.Dense(d_k, use_bias=False, activation='linear') for _ in range(self.num_heads)]\n",
        "        self.Wv = [keras.layers.Dense(d_k, use_bias=False, activation='linear') for _ in range(self.num_heads)]\n",
        "        # Final weight matrix applied to concatenated output of all heads\n",
        "        self.W0 = keras.layers.Dense(d_model, use_bias=False, activation='linear')\n",
        "        # Softmax\n",
        "        self.softmax = keras.layers.Softmax()\n",
        "        # Layer normalisation\n",
        "        self.layernorm = keras.layers.LayerNormalization()\n",
        "        # Dropout\n",
        "        self.dropout = keras.layers.Dropout(0.1)\n",
        "        # Addition layer\n",
        "        self.add = keras.layers.Add()\n",
        "\n",
        "    # We take three inputs here, the values that will be projected into q, k and v\n",
        "    # For self-attention these are all the same, but for cross-attention q comes\n",
        "    # the masked attention, and k and v from the encoder.\n",
        "    def call(self, q, k, v, mask=None):\n",
        "        # Z will store the output from each head in a list\n",
        "        Z = []\n",
        "        # Loop over all heads\n",
        "        for head in range(self.num_heads):\n",
        "            # Q, K and V projections. Pass the correct inputs through the\n",
        "            # corresponding dense layers by replacing \"None\"\n",
        "            Q = self.Wq[head](q)\n",
        "            K = self.Wk[head](k)\n",
        "            V = self.Wv[head](v)\n",
        "            # Multiply Q by the transpose of K\n",
        "            QKT = tf.matmul(Q, K, transpose_b=True)\n",
        "            # The mask is added to QKT at this point. This None is actually\n",
        "            # part of the code, and not something to change!\n",
        "            if mask is not None: # DON'T CHANGE THIS NONE!\n",
        "                QKT = self.add([QKT, mask])\n",
        "            # Now we apply the normalisation\n",
        "            QKT = QKT / tf.math.sqrt(float(self.d_k))\n",
        "            # Apply the softmax activation to get the attention matrix\n",
        "            A = self.softmax(QKT)\n",
        "            A = self.dropout(A)\n",
        "            # Final output for each head. We need to multiply matrices A and V\n",
        "            att_output = tf.matmul(A, V)\n",
        "            # Add the result for this head to the list of results\n",
        "            Z.append(att_output)\n",
        "\n",
        "        # Concatenate outputs from the heads\n",
        "        concZ = tf.concat(Z, axis=-1)\n",
        "        # Mutliply by the final weight matrix W0 to give us a shape equal to the input\n",
        "        output = self.W0(concZ)\n",
        "        # Perform some dropout\n",
        "        output = self.dropout(output)\n",
        "        # Make the residual connection that connects input q to the output and\n",
        "        # normalise the weights\n",
        "        output = self.layernorm(self.add([q,output]))\n",
        "        return output\n",
        "\n"
      ],
      "metadata": {
        "id": "7jRUM3OZj533"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "Since this is the most important part of the transformer, let's perform a quick test to see if we get the expected answer. If your attention layer is correct then you will see the following output:\n",
        "\n",
        "A tensor of shape `(1, 5, 4)` with the following values:\n",
        "```\n",
        "[[[-1.2209358   1.5630928  -0.19064169 -0.15151532]\n",
        "  [-1.2343581   1.1889216  -0.7027581   0.7481946 ]\n",
        "  [-0.8607208   1.3911251  -1.0319505   0.5015463 ]\n",
        "  [-0.5319073   1.7108996  -0.7824128  -0.39657927]\n",
        "  [-1.1439677   1.6047997  -0.25109336 -0.20973878]]]\n",
        "```\n",
        "As expected, the output from the layer has the same shape as the input.\n"
      ],
      "metadata": {
        "id": "EsyJZXxYYZFK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is like a unit test of the layer\n",
        "keras.utils.set_random_seed(42) # Reset the seed for network weight initialisation\n",
        "test_model_input = keras.layers.Input(shape=(5,4), dtype=\"float32\")\n",
        "test_model_output = CustomAttention(d_model=4, d_k=3, num_heads=8)(test_model_input, test_model_input, test_model_input, mask=None)\n",
        "test_model = keras.Model(test_model_input, test_model_output)\n",
        "unit_test_1_output = test_model.predict(unit_test_1_input)\n",
        "print(unit_test_1_output.shape)\n",
        "print(unit_test_1_output)"
      ],
      "metadata": {
        "id": "m4aP_v4c7wQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Now we create the feed forward network. This is a very simple network consisting of just two `Dense` layers. The first layer expands the dimensions of the input to `ff_dim` and the second contracts it back down to the size of the input, `d_model`. Here the original transformer uses a `relu` activation on the first layer, and no (the same as `linear`) activation in the second."
      ],
      "metadata": {
        "id": "Oljz0d7_gh0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feed forward network including residual connection. This is just a two layer\n",
        "# neural network using dense layers\n",
        "class FeedForward(keras.layers.Layer):\n",
        "    def __init__(self, d_model, ff_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        # Dense layers - fill in the number of neurons\n",
        "        self.dense_1 = keras.layers.Dense(ff_dim, activation='relu')\n",
        "        self.dense_2 = keras.layers.Dense(d_model, activation='linear')\n",
        "        # Layer normalisation\n",
        "        self.layernorm = keras.layers.LayerNormalization()\n",
        "        # Dropout\n",
        "        self.dropout = keras.layers.Dropout(0.1)\n",
        "        # Addition layer\n",
        "        self.add = keras.layers.Add()\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # First dense layer followed by some dropout\n",
        "        output = self.dense_1(inputs)\n",
        "        output = self.dropout(output)\n",
        "        # Second dense layer followed by some dropout\n",
        "        output = self.dense_2(output)\n",
        "        output = self.dropout(output)\n",
        "        # Perform the residual connection and normalise\n",
        "        output = self.layernorm(self.add([inputs,output]))\n",
        "        return output"
      ],
      "metadata": {
        "id": "DR-B-9Cgeyef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Let's add a second little test. We'll pass the output of the attention through the feed forward layer and see what we get.\n",
        "A tensor with shape `(1, 5, 4)` and values:\n",
        "```\n",
        "[[[ 1.7156274  -0.572421   -0.4953011  -0.64790493]\n",
        "  [ 0.6662822   1.2113216  -1.3278834  -0.54972064]\n",
        "  [ 1.4146247  -0.65186036  0.40717575 -1.1699401 ]\n",
        "  [-0.54764104  1.7212923  -0.6902877  -0.48336366]\n",
        "  [ 0.8909051  -1.0794511   1.0144458  -0.8259004 ]]]\n",
        "  ```\n",
        "   Again, the output size will match the original input size, and the elements should be as above\n"
      ],
      "metadata": {
        "id": "yuBInHU99gid"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keras.utils.set_random_seed(42) # Reset the seed for network weight initialisation\n",
        "test_model_2_input = keras.layers.Input(shape=(5,4), dtype=\"float32\")\n",
        "test_model_2_output = FeedForward(d_model=4, ff_dim=20)(test_model_2_input)\n",
        "test_model_2 = keras.Model(test_model_2_input, test_model_2_output)\n",
        "unit_test_2_output = test_model_2.predict(unit_test_2_input)\n",
        "print(unit_test_2_output.shape)\n",
        "print(unit_test_2_output)"
      ],
      "metadata": {
        "id": "9g-xOpmn9hsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Now we have our building blocks, we just need to put them together to make the encoder and decoder. The encoder is now very simple - we just need to perform two steps:\n",
        "1. Self-attention: The same input is used for the  `𝑞` ,  `𝑘`  and  `𝑣`  inputs to the `CustomAttention` layer we defined earlier. We need to make sure to pass on the `mask` here too.\n",
        "2. The output from the self-attention goes through the `FeedForward(d_model, ff_dim)` layer.\n",
        "\n",
        "Here you need to define the `self.attention` and `self.feedforward` class variables using the constructors of `CustomAttention` and `FeedForward` that we defined above, making sure to pass the necessary parameters to them, such as `d_model`.\n",
        "\n",
        "Then in the `call` function we actually call the layers and apply them."
      ],
      "metadata": {
        "id": "avWTp6g6FYyv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we have the building blocks that we need, let's make our Encoder\n",
        "class Encoder(keras.layers.Layer):\n",
        "    def __init__(self, d_model, d_k, ff_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        # Attention - fill in the correct inputs\n",
        "        self.attention = CustomAttention(d_model, d_k, num_heads)\n",
        "        # Feed-forward network - fill in the correct inputs\n",
        "        self.feedforward = FeedForward(d_model, ff_dim)\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        # Call the attention function with the correct arguments. Fill in the\n",
        "        # values for q, k and v.\n",
        "        z = self.attention(q=inputs, k=inputs, v=inputs, mask=mask)\n",
        "        # Feed forward network applied to z (the output of the attention)\n",
        "        output = self.feedforward(z)\n",
        "        return output"
      ],
      "metadata": {
        "id": "DzRdvtCkj_ja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test our encoder now. As before, we should get a tensor with shape `(1, 5, 4)` with the exact values:\n",
        "```\n",
        "[[[-1.2161773   1.3127743  -0.67634934  0.5797525 ]\n",
        "  [-1.1994      1.5463303  -0.40336332  0.05643302]\n",
        "  [-1.6334743   0.9733443   0.6144269   0.04570324]\n",
        "  [-1.0999652   1.5638233  -0.57305396  0.10919581]\n",
        "  [-1.2223432   1.4977748  -0.46958745  0.19415595]]]\n",
        "```"
      ],
      "metadata": {
        "id": "ZLzj6M4Biro9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Unit test for the encoder\n",
        "keras.utils.set_random_seed(42) # Reset the seed for network weight initialisation\n",
        "test_model_3_input = keras.layers.Input(shape=(5,4), dtype=\"float32\")\n",
        "test_model_3_output = Encoder(d_model=4, d_k=3, ff_dim=20, num_heads=8)(test_model_3_input, mask=None)\n",
        "test_model_3 = keras.Model(test_model_3_input, test_model_3_output)\n",
        "unit_test_3_output = test_model_3.predict(unit_test_3_input)\n",
        "print(unit_test_3_output.shape)\n",
        "print(unit_test_3_output)"
      ],
      "metadata": {
        "id": "6muXdngPio_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "The decoder is marginally more complex than the encoder, since we need to have two attention layers:\n",
        "1. Masked self-attention: input to the decoder is used for the `𝑞`, `𝑘` and `𝑣` inputs to the `CustomAttention` layer we defined earlier. We need to make sure we are using the mask for the decoder input here.\n",
        "2. Cross-attention: this uses the output of the encoder as the $k$ and $v$ inputs, but the output of the masked attention as $q$. We use the `CustomAttention` layer again here, making sure to use the mask that we created for the encoder input.\n",
        "3. The output is passed into a feed-forward network to give the final ouput of the decoder, using the `FeedForward` layer.\n"
      ],
      "metadata": {
        "id": "AL9-Tp9qF8zl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let's make the decoder.\n",
        "class Decoder(keras.layers.Layer):\n",
        "    def __init__(self, d_model, d_k, ff_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        # Attention, both masked and cross. Fill in the entire line this time,\n",
        "        # following what was shown for the Encoder\n",
        "        self.masked_attention = CustomAttention(d_model, d_k, num_heads)\n",
        "        self.cross_attention = CustomAttention(d_model, d_k, num_heads)\n",
        "        # Feed-forward network\n",
        "        self.feedforward = FeedForward(d_model, ff_dim)\n",
        "\n",
        "    # We have the encoder output to include here\n",
        "    def call(self, inputs, encoder_output, decoder_mask=None, encoder_mask=None):\n",
        "        # Call the masked attention function with the correct inputs and mask\n",
        "        z = self.masked_attention(q=inputs, k=inputs, v=inputs, mask=decoder_mask)\n",
        "        # Call the cross-attention function with the correct inputs and mask\n",
        "        c = self.cross_attention(q=z, k=encoder_output, v=encoder_output, mask=encoder_mask)\n",
        "        # Feed forward network applied to the output of the cross-attention\n",
        "        output = self.feedforward(c)\n",
        "        return output"
      ],
      "metadata": {
        "id": "tHL1Sd9ethSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A final unit test. Let's make sure that the decoder is working correctly. We'll use the output from the test of the encoder and a random input. Again, the output tensor should have shape `(1, 5, 4)` with values\n",
        "```\n",
        "[[[-1.0793793   1.539169    0.19299026 -0.6527799 ]\n",
        "  [-1.1096522   1.6228733  -0.26464212 -0.24857906]\n",
        "  [-1.0544202   1.5797127   0.09240623 -0.6176987 ]\n",
        "  [-1.1864818   1.5654144  -0.00610236 -0.37283033]\n",
        "  [-0.7599277   1.6016579   0.07923051 -0.92096066]]]\n",
        "```"
      ],
      "metadata": {
        "id": "wxSVOQmIXPRC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Unit test for the decoder. This should give the result above\n",
        "keras.utils.set_random_seed(42) # Reset the seed for network weight initialisation\n",
        "test_model_4_input = keras.layers.Input(shape=(5,4), dtype=\"float32\")\n",
        "test_model_4_input_2 = keras.layers.Input(shape=(5,4), dtype=\"float32\")\n",
        "test_model_4_output = Decoder(d_model=4, d_k=3, ff_dim=20, num_heads=8)(test_model_4_input, test_model_4_input_2)\n",
        "test_model_4 = keras.Model((test_model_4_input, test_model_4_input_2), test_model_4_output)\n",
        "unit_test_4_output = test_model_4.predict([unit_test_4_input, unit_test_3_output])\n",
        "print(unit_test_4_output.shape)\n",
        "print(unit_test_4_output)"
      ],
      "metadata": {
        "id": "yJQSoNSbXlAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Now all we need to do is put the building blocks together to create the full transformer model.\n",
        "1. Create the three masks that we need to make use of:\n",
        "> 1.   Padding mask for the encoder input\n",
        "> 2.   Padding mask for the decoder input\n",
        "> 3.   Causal mask for the decoder (prevents us looking into the future when predicting the output)\n",
        "2. Perform the sentence embedding for encoder (English) and decoder (Spanish) inputs\n",
        "3. Call the encoder with the English input and mask\n",
        "4. Call the decoder with the Spanish input, theoutput from the encoder, the combination of the decoder masks and the encoder mask\n",
        "5. Make the predictions from the output of the decoder\n",
        "6. Sit back and enjoy some translations soon"
      ],
      "metadata": {
        "id": "kI4uoO6xD1Jg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Finally we build the transformer\n",
        "class Transformer(keras.layers.Layer):\n",
        "    def __init__(self, sequence_length, d_model, d_k, ff_dim, num_heads, vocab_size_english, vocab_size_spanish, learned_encoding, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        # The sequence lengths for the encoder and decoder inputs\n",
        "        self.l_e = sequence_length\n",
        "        self.l_d = sequence_length + 1\n",
        "        # Layers to encode the inputs for the encoder and decoder\n",
        "        self.english_embed = EmbedAndEncode(self.l_e, d_model, vocab_size_english, learned_encoding)\n",
        "        self.spanish_embed = EmbedAndEncode(self.l_d, d_model, vocab_size_spanish, learned_encoding)\n",
        "        # The encoder and decoder\n",
        "        self.encoder = Encoder(d_model, d_k, ff_dim, num_heads)\n",
        "        self.decoder = Decoder(d_model, d_k, ff_dim, num_heads)\n",
        "        self.dropout = keras.layers.Dropout(0.1)\n",
        "        # The final output layer to make the preditions\n",
        "        self.classifier = keras.layers.Dense(vocab_size_spanish, activation='softmax')\n",
        "\n",
        "    def call(self, enc_in, dec_in):\n",
        "        # We need to create masks before making the embeddings to prevent the\n",
        "        # attention mechanism from considering padded values\n",
        "        encoder_padding_mask = self.create_padding_mask(enc_in)\n",
        "        decoder_padding_mask = self.create_padding_mask(dec_in)\n",
        "        decoder_causal_mask = self.create_causal_mask()\n",
        "        # Combine the two masks for the decoder\n",
        "        decoder_mask = tf.minimum(decoder_padding_mask, decoder_causal_mask)\n",
        "        # Prepare the encoder input and run it\n",
        "        enc_in = self.english_embed(enc_in)\n",
        "        enc_in = self.dropout(enc_in)\n",
        "        enc_out = self.encoder(enc_in, encoder_padding_mask)\n",
        "        # Prepare the decoder input and run it\n",
        "        dec_in = self.spanish_embed(dec_in)\n",
        "        dec_in = self.dropout(dec_in)\n",
        "        dec_out = self.decoder(dec_in, enc_out, decoder_mask, encoder_padding_mask)\n",
        "        # Make the predictions\n",
        "        dec_out = self.dropout(dec_out)\n",
        "        dec_out = self.classifier(dec_out)\n",
        "        return dec_out\n",
        "\n",
        "    def create_causal_mask(self):\n",
        "        # The causal mask is an upper triangular matrix with values set\n",
        "        # to a very large negative number\n",
        "        mask = np.triu(np.ones((self.l_d, self.l_d)) * -1.0e20, k=1)\n",
        "        return mask\n",
        "\n",
        "    def create_padding_mask(self, inputs):\n",
        "        # Determine which positions are non-zero\n",
        "        mask = tf.math.equal(inputs, 0)\n",
        "        mask = tf.cast(mask, tf.float64)\n",
        "        # Ensure the mask is flexible enough to broadcast over different size matrices\n",
        "        mask = mask[:, tf.newaxis, :]\n",
        "        # Set the values we want to mask to a very large negative number\n",
        "        mask *= -1.0e20\n",
        "        return mask"
      ],
      "metadata": {
        "id": "SGTu4__K-5J2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Now lets define the parameters of the model and build it. To keep things reasonably sized. At some point you could try changing these if you have a GPU that you can use for training, but you will need to use these values to load my model weights later.\n",
        "\n",
        "*   $d_\\textrm{model} = 256$\n",
        "*   $n_\\textrm{heads} = 8$\n",
        "*   $d_k = 32$\n",
        "*   $\\textrm{ff_dim} = 1024$\n",
        "\n",
        "In this example I have set $d_k = d_v$, so we won't see $d_v$ in this code. This follows the method used in the original transformer, but you could modify the layers above to allow for a different value of $d_v$\n",
        "\n",
        "Using these values, you should find that the network has `13385624` parameters\n"
      ],
      "metadata": {
        "id": "GkJVpCIsFfWw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill in the model parameter values. Use those given about if you want to be\n",
        "# able to load that weights from my trained network later\n",
        "d_model = 256\n",
        "num_heads = 8\n",
        "d_k = 32\n",
        "ff_dim = 1024\n",
        "\n",
        "# Define the two input layers that we need for the model\n",
        "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
        "\n",
        "transformer_outputs = Transformer(sequence_length, d_model, d_k, ff_dim, num_heads, vocab_size_english, vocab_size_spanish, learned_encoding=True)(encoder_inputs,decoder_inputs)\n",
        "transformer = keras.Model([encoder_inputs,decoder_inputs], transformer_outputs)\n",
        "transformer.summary()\n"
      ],
      "metadata": {
        "id": "Qm9TiiHP2iCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can use a learning rate scheduler to reduce the learning rate if necessary\n",
        "# Not really necessary here as it will take quite a few epochs to be useful,\n",
        "# but I leave it here for completeness\n",
        "lr_schedule = keras.callbacks.ReduceLROnPlateau(patience=3)\n",
        "\n",
        "# Use the optimiser settings from the original paper (but not the lr mechanism)\n",
        "optimiser = keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.98, epsilon=1.0e-9)\n",
        "\n",
        "# This custom loss function helps us to converge better. We ignore the padding class\n",
        "# in the loss function and smooth the labels a little. It would still work ok\n",
        "# just using the standard keras.losses.SparseCategoricalCrossentropy() loss\n",
        "def masked_and_padded_scce_loss(y_true, y_pred, label_smoothing=0.1):\n",
        "    # Convert from a sparse to a one-hot representation of the truth\n",
        "    y_true_one_hot = tf.one_hot(tf.cast(y_true, tf.int32), depth=tf.shape(y_pred)[-1])\n",
        "    # We want to ignore the padding class, which is the first element of the one-hot vector\n",
        "    smoothed_loss = tf.keras.losses.categorical_crossentropy(\n",
        "        y_true_one_hot[:,:,1:], y_pred[:,:,1:], from_logits=False,\n",
        "        label_smoothing=label_smoothing)\n",
        "    return smoothed_loss\n",
        "\n",
        "# Compile the model ready for training\n",
        "transformer.compile(optimiser, loss=masked_and_padded_scce_loss, metrics=[\"accuracy\"])\n"
      ],
      "metadata": {
        "id": "E8oYLo6epOkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Now we can train the network! Unfortunately this will be rather slow so in the next block you can load some weights from a network I trained previously. One a V100 it trains very quickly - less than 1 minute per epoch! If you have a GPU available, go ahead and train it for 10 epochs or so, otherwise feel free to skip this block\n"
      ],
      "metadata": {
        "id": "WyIYlJMUeDxh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we can actually train our transformer!\n",
        "num_epochs = 1\n",
        "transformer.fit(training_dataset, epochs=num_epochs, validation_data=validation_dataset, callbacks=[lr_schedule])\n",
        "# Save the weights if you want to keep them\n",
        "transformer.save_weights(\"my_transformer_weights.keras\")"
      ],
      "metadata": {
        "id": "7My5RhbFO9Xi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Training is unfortunately very slow on a CPU, so if you don't have any GPU access then feel free to load some model weights that I produced earlier\n"
      ],
      "metadata": {
        "id": "1ytwdb1rdz2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instead of training, we can load my weights\n",
        "!wget --no-check-certificate 'https://www.hep.phy.cam.ac.uk/~lwhitehead/lhw_weights.keras' -O lhw_weights.keras\n",
        "transformer.load_weights(\"lhw_weights.keras\")"
      ],
      "metadata": {
        "id": "6jlNSoJyqWyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "Now all we need to do is look at how to perform inference. As I said in the lectures, this is done in an iterative way. We pass the english sentence into the encoder, and then pass the `<sos>` (start of sentence) token to the decoder. The decoder then predicts the output word, which we then append to the `<sos>` token and run things again with this new input."
      ],
      "metadata": {
        "id": "LeanDcqZ6uiZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Get our vocabulary from the vectorisation layer we made right back at the top\n",
        "# of the notebook\n",
        "spanish_vocab = spanish_vectorisation.get_vocabulary()\n",
        "spa_index_lookup = dict(zip(range(len(spanish_vocab)), spanish_vocab))\n",
        "\n",
        "def run_inference(input_sentence):\n",
        "    # Get the tokenised input sentence ready for embedding\n",
        "    encoded_input = english_vectorisation([input_sentence])\n",
        "    # The input to the decoder is initially just \"<sos>\". In the second\n",
        "    # iteration the input becomes \"<sos> <first_predicted_word>\"\n",
        "    decoded_sentence = \"<sos>\"\n",
        "    for i in range(sequence_length):\n",
        "        decoder_input = spanish_vectorisation([decoded_sentence])\n",
        "        predictions = transformer([encoded_input, decoder_input])\n",
        "        # Get the index of the word with the highest probability. The\n",
        "        # index i here ensures we look at the correct row of the output\n",
        "        # since it has size (1, len(decoded_sentence), vocab_size_spanish)\n",
        "        predicted_word_index = np.argmax(predictions[0, i, :])\n",
        "        predicted_word = spa_index_lookup[predicted_word_index]\n",
        "        decoded_sentence += \" \" + predicted_word\n",
        "\n",
        "        if predicted_word == \"<eos>\":\n",
        "            break\n",
        "    # Print the input and output, removing the <sos> and <eos> tokens\n",
        "    print(input_sentence,\" :: \",decoded_sentence[6:-6])"
      ],
      "metadata": {
        "id": "QZtnPxHPWE8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now for the fun part! I've added a few sentences for the transformer to try to translate. The first sentence is, of course, the one from the lectures. Feel free to play around and add your own sentences and see how it does (if you can't speak spanish, you could ask Lorena, Zahari, Google, and in a crisis, me.)\n",
        "\n",
        "If you've loaded my weights, then the five sentences below are translated correctly. As a bit of a language technicality, in the lectures I said that `I have a big cat` should translate to `Yo tengo un gato grande`. You may notice that `Yo`, meaning `I`, is missing - pronouns such as `yo` are often dropped from spanish since the conjugation of the verb (`tengo`) tells you that the pronoun is `yo` anyway."
      ],
      "metadata": {
        "id": "T-51tDxng0Hr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_inference(\"i have a big cat\")\n",
        "run_inference(\"i have a small dog\")\n",
        "run_inference(\"are there horses here\")\n",
        "run_inference(\"my car is broken\")\n",
        "run_inference(\"im going to translate this sentence\")"
      ],
      "metadata": {
        "id": "t7UABLDR6ASt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KITb7S_EvlmG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}